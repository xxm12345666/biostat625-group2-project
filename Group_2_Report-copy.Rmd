---
title: "Research on Pneumonia Patient Condition Classification Using Diffusion Models and CLIP"
author: "Xiaomeng Xu; Wenfei Mao; Yingzhen Wang; Shuoyuan Gao"
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: xelatex
    number_sections: true
  word_document: default
bibliography: references.bib
header-includes:
- \usepackage{graphicx}
- \usepackage{amsmath}
- \usepackage{booktabs}
- \usepackage{geometry}
- \geometry{a4paper, margin=1in}
- \setlength{\parindent}{0em}
- \setlength{\parskip}{1em}
---
\section*{Abstract}

Pneumonia remains a leading cause of childhood mortality worldwide. This study explores large language model techniques to classify pediatric chest X-ray images into normal, bacterial pneumonia, and viral pneumonia categories. A dataset of 5,856 radiographs was used, with synthetic images generated via LoRA fine-tuning of a Stable Diffusion model to address data imbalance. A fine-tuned CLIP model achieved modest improvements in training accuracy (from 48.94% to 50.51%), while the test accuracy remained at 37.50%.

# Introduction

## Background

Pneumonia remains the leading cause of death worldwide among children under five years of age. Despite the implementation of safe, effective, and affordable interventions that have significantly reduced pneumonia mortality from 4 million in 1981 to just over one million in 2013, pneumonia still accounts for nearly one-fifth of all childhood deaths globally [@who_pneumonia]. Chest X-ray imaging has emerged as a promising modality for radiologic diagnosis of pneumonia. However, its role in clinical management and its impact on patient outcomes require further optimization [@kermany2018pneumonia].

## Dataset

The dataset utilized for this study comprised 5,856 pediatric chest X-ray images categorized into three classes: Normal, Virus and Bacteria. The images were collected from 5,856 patients aged one to five years, all receiving clinical care at the Guangzhou Women and Children’s Medical Center. To ensure data quality and reliability, all radiographs underwent preprocessing to eliminate low-quality scans. Furthermore, the images were independently reviewed and classified by two specialist physicians and validated by a third-party expert to minimize the risk of misclassification [@dincer_chestxray].

## Imaging Features

The dataset includes radiographic images demonstrating distinct features associated with different lung conditions [@kermany2018pneumonia]:

-	Normal Lungs: These images show no pathological changes.
-	Bacterial Pneumonia: Characterized by localized consolidation with well-defined margins, often accompanied by pleural effusion.
-	Viral Pneumonia: Identified by bilateral ground-glass opacities, reticular patterns, or patchy infiltrates with poorly defined borders, reflecting its diffuse and interstitial nature.

These imaging features provide critical diagnostic insights and support the development of effective treatment strategies for pneumonia.

## Queation of Interest

Our study investigates the effectiveness of fine-tuning the Stable Diffusion model with LoRA for synthetic image generation and leveraging a CLIP model for robust classification of three chest X-ray categories, as well as exploring their potential limitations and future improvements in pediatric radiologic diagnosis.


# Method
## Diffusion Model

Diffusion models draw inspiration from non-equilibrium thermodynamics. They employ a Markov chain of diffusion steps to gradually introduce random noise into the data and then learn to reverse this process, reconstructing desired data samples from the noise. These models are trained using a fixed procedure, and the latent variables operate in a high-dimensional space. Diffusion models consist of two processes: the forward diffusion process and the reverse diffusion process. In the forward process, given a data point sampled from the real data distribution, $x_0 \sim q(\mathbf{x})$, we define a forward diffusion process that progressively adds small amounts of Gaussian noise to the sample over $T$ steps. This process generates a sequence of noisy samples, $x_1, \ldots, x_T$. The step sizes are determined by a variance schedule ${\beta_t \in (0, 1)}_{t=1}^{T}$. The data sample would generally lose its distinguishable features as the step becomes larger. Eventually, $x_T$ would equivalent to an isotropic Gaussian distribution. we will be able to recreate the true sample from a Gaussian noise input, $\mathbf{x}_{T} \sim \mathcal{N}(0, \mathbf{I})$.[@ho2020denoisingdiffusionprobabilisticmodels] 

Firstly, in our project, we deployed the stable Diffusion Version 2. The model use 865M U-Net as image generator and use OpenCLIP ViT-H/14 as image-text encoder and it could generate 768×768px outputs. Then we utilized LoRA to fined-tuned the model to make the model generate customized images. Specifically, we fine-tuned attention layer and projection layer of U-Net. LoRA keeps the pre-trained model weights fixed while incorporating trainable low-rank decomposition matrices into each layer of the Transformer architecture. It could significantly reduce the number of trainable parameters required for the downstream tasks. It could also reduce the GPU requirement by 3 times.[@hu2021loralowrankadaptationlarge] Finally, after fine-tuning the model, we used it to generate 1,000 synthetic images for normal lung images and viral pneumonia images separately to address the problem of data imbalance. Finally, we combined the original dataset and the synthetic dataset for training the CLIP model.

## Contrastive Language-Image Pre-training(CLIP) 

After combined the original and synthetic dataset, we used these data to fine-tuned the projection layer and full connected layer of CLIP model with LoRA. CLIP is a neural network trained on a variety of (image,text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, similarly to the capabilities of GPT-2 and GPT-3.[@radford2021learningtransferablevisualmodels] In our project, we deployed CLIP-ViT-large-patch14. The model use ViT-L/14 Transformer as image encoder and use masked self-attention Transformer as text encoder. CLIP model was pre-trained on a larger-scale dataset. Therefore, it could achieve very impressive results on many computer vision tasks. However, it would achieve much better results after fine-tuning, because our dataset is a medical dataset. After fine-tuning the CLIP model, we used fine-tuned model to classify test dataset into three categories. The basic principle of image classification in the CLIP model is that it calculates the cosine similarity between images and text. The image and text with the highest similarity are matched, thus completing the classification. Therefore, we need to create some prompts to use for classification. We tried different prompts and found that 'An image of [Type] chest X-ray,' where the type could be one of three: normal, bacteria, or virus, worked best. Finally, we successfully used the fine-tuned model to classify the test dataset.

## Experiment Setup

Our experiments were conducted on a machine equipped with an NVIDIA RTX 3090 GPU with 24GB memory, running CUDA 12.2 Toolkit. The model was implemented using PyTorch 2.5.1 and Python 3.9.19. We fine-tuned the CLIP model for 3 epochs using the AdamW optimizer, with a batch size of 4 and a learning rate of 5e-5. For parameter-efficient fine-tuning, we adopted LoRA with a configuration of LoRA alpha = 32 and LoRA dropout = 0.1. More detailed parameter settings can be found on Github.

```{r setup, include=FALSE}
# Setup chunk
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(ggplot2)
library(knitr)
```


# Result

```{r}
library(ggplot2)
library(tidyr)

results <- data.frame(
  Epoch = c('Epoch1', 'Epoch2', 'Epoch3'),
  Train_Accuracy = c(0.4894, 0.5051, 0.5047),
  Train_F1_Score = c(0.4725, 0.4911, 0.4891),
  Train_Recall = c(0.4865, 0.5095, 0.5056)
)

results_long <- results %>%
  pivot_longer(cols = -Epoch, names_to = "Metric", values_to = "Value")

plot <- ggplot(results_long, aes(x = Epoch, y = Value, color = Metric, group = Metric)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  labs(
    title = "Training Performance Across Epochs",
    x = "Epoch",
    y = "Value",
    color = "Metrics"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.position = "bottom"
  )

plot
cat("Figure 1: Training performance metrics (Accuracy, F1-Score, Recall) across epochs.\n")
```

From the result, we can see the image shows that as the number of training epochs increases, the training accuracy, recall, and F1 score gradually improve. The accuracy rises from 0.4894 to 0.5047, recall increases from 0.4865 to 0.5056, and the F1 score grows from 0.4725 to 0.4891. This indicates that the model is progressively learning the features of the training images. The final accuracy on the test dataset is 0.375, and the recall is 0.5, indicating that approximately half of the images in the test set were correctly classified.

# Conclusion

From the above results, we can see that the final classification outcome is promising. Our project leverages a multimodal large model approach, which, compared to traditional CNN networks, fully utilizes information from different modalities, thereby achieving an ideal image classification performance. Moreover, multimodal LLMs possess more powerful feature representation capabilities, enabling them to learn finer-grained feature representations, which allows for better handling of complex tasks. Compared to CNNs, multimodal LLMs exhibit stronger generalization abilities and better interpretability, making the classification results easier to understand and explain.

While the proposed approach demonstrates promising results, it still has certain limitations, and we propose some future work to address these limitations. Firstly, the stable diffusion model uses DDPM (Denoising Diffusion Probabilistic Models) for training. The process requires approximately 1,000 steps to complete, which is time-consuming and has high GPU requirements. However, the consistency model[@song2023consistencymodels] requires only 5 steps or fewer to complete this process because it follows a certain route. This makes it up to 50 times faster compared to the stable diffusion model. Recently, at CVPR 2024, UC Berkeley [@frans2024stepdiffusionshortcutmodels] proposed a one-step diffusion model. Therefore, there are many other promising approaches to explore in future work. Secondly, after fine-tuning the model, we can apply reinforcement learning methods, such as DPO or PPO, to further optimize the model. Reinforcement learning methods has achieved great success in post-training stage of large language model. For example, OpenAI's recent o1 model has been optimized using reinforcement learning and has significantly outperformed GPT-4 in multiple tasks. Therefore, we could adopt such methods to further improve the classification results. Finally, we adopted LoRA to fine-tune the model due to limited computing resources. However, it may introduce noise to the training dataset because it cannot handle more customized images. Therefore, in future work, full parameters fine-tuning or reasoning with reinforced fine-tuning [@luong2024reftreasoningreinforcedfinetuning] may be better options compared to LoRA, as they can achieve better fine-tuning results and reduce the impact on the training dataset.


# Contribution
Xiaomeng Xu: Code editing; abstract; introduction
Wenfei Mao: Code editing; Diffusion Model; CLIP; Conclusion
Yingzhen Wang: Code editing; Results; Diffusion model
Shuoyuan Gao: Code editing; Experiment Setup; Conclusion

Github Link: 

\section*{References}